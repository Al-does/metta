agent:
  _target_: agent.metta_agent.MettaAgent
  observation_encoder:
    _target_: agent.simple_encoder.SimpleConvAgent
    cnn_channels: 32
    obs_key: grid_obs
    normalize_features: true
  fc:
    layers: 1
    output_dim: 128
  decoder:
    _target_: agent.decoder.Decoder
  core:
    rnn_type: gru
    rnn_num_layers: 1
    rnn_size: 128
env:
  name: GDY-MettaGrid
  _target_: mettagrid.mettagrid_env.MettaGridEnv
  report_stats_interval: 100
  normalize_rewards: true
  hidden_features:
    grid_obs:
    - agent:energy
    - agent:hp
  game:
    obs_width: 11
    obs_height: 11
    max_steps: 1000
    tile_size: 16
    num_agents: 20
    no_energy_steps: 500
    objects:
      agent:
        initial_energy: 250
        max_energy: 250
        max_inventory: 5
        freeze_duration: 10
        energy_reward: false
        hp: 10
        mortal: false
        upkeep.time: 0
        upkeep.shield: 1
        use_cost: 0
      altar:
        hp: 30
        cooldown: 2
        use_cost: 100
      converter:
        hp: 30
        cooldown: 2
        energy_output.r1: 100
        energy_output.r2: 10
        energy_output.r3: 1
      generator:
        hp: 30
        cooldown: 5
        initial_resources: 30
        use_cost: 0
      wall:
        hp: 10
    actions:
      noop:
        enabled: true
        cost: 0
      move:
        enabled: true
        cost: 0
      rotate:
        enabled: true
        cost: 0
      jump:
        enabled: false
        cost: 3
      shield:
        enabled: true
        cost: 1
      transfer:
        enabled: false
        cost: 0
      use:
        enabled: true
        cost: 0
      attack:
        enabled: true
        cost: 5
        damage: 5
      gift:
        enabled: false
        cost: 0
    map:
      layout:
        rooms_x: 1
        rooms_y: 1
        rooms:
        - - base
          - wild_1
          - base
        - - wild_2
          - center
          - wild_2
        - - base
          - wild_1
          - base
      room:
        width: 25
        height: 25
        num_agents: 5
        objects:
          agent: 5
          altar: 1
          converter: 3
          generator: 15
          wall: 10
      wild_1:
        width: 10
        height: 15
        border: 0
        objects:
          agent: 0
          altar: 1
          converter: 1
          generator: 5
          wall: 5
      wild_2:
        width: 15
        height: 10
        border: 0
        objects:
          agent: 0
          altar: 1
          converter: 1
          generator: 5
          wall: 5
      center:
        width: 10
        height: 10
        border: 0
        objects:
          agent: 0
          altar: 2
          converter: 5
          generator: 10
          wall: 20
      base:
        width: 15
        height: 15
        border: 1
        objects:
          agent: 5
          altar: 1
          converter: 3
          generator: 5
          wall: 5
  kinship:
    team_size: 1
    team_reward: 0
train:
  num_envs_per_worker: ???
  num_workers: 1
  init_policy_uri: null
  resume: true
  env: ${env.name}
  exp_id: ${experiment}
  seed: 1
  cpu_offload: false
  total_timesteps: 100000
  learning_rate: 0.0012654662762525116
  num_steps: 32
  anneal_lr: false
  gamma: 0.7755949999823327
  gae_lambda: 0.9061454965924297
  update_epochs: 1
  norm_adv: true
  clip_coef: 0.059061983960164655
  clip_vloss: true
  ent_coef: 0.005792010009804944
  vf_coef: 0.6632257135848436
  max_grad_norm: 0.07688769698143005
  target_kl: null
  env_batch_size: null
  zero_copy: true
  verbose: true
  checkpoint_interval: 10
  wandb_checkpoint_interval: 300
  batch_size: 1024
  minibatch_size: 1024
  bptt_horizon: 8
  vf_clip_coef: 0.0008792699500449386
  compile: false
  compile_mode: reduce-overhead
  forward_pass_minibatch_target_size: 2
  async_factor: 1
wandb:
  enabled: true
  track: enabled
  project: metta
  entity: metta-research
  group: metta
  name: ${experiment}
sweep:
  metric: action.use.altar
  resume: true
  eval:
    num_envs: 10
    num_episodes: 10
    max_time_s: 5
    policy_agents_pct: 1.0
    policy_uri: null
    baseline_uris: []
  parameters:
    env:
      normalize_rewards:
        space: linear
        is_int: true
        min: 0
        max: 1
    train:
      learning_rate:
        space: log
        min: 1.0e-05
        max: 0.1
      gamma:
        space: logit
        min: 0.0
        max: 1.0
      gae_lambda:
        space: logit
        min: 0.0
        max: 1.0
      update_epochs:
        space: linear
        is_int: true
        min: 1
        max: 16
      clip_coef:
        space: logit
        min: 0.0
        max: 1.0
      vf_coef:
        space: logit
        min: 0.0
        max: 1.0
      vf_clip_coef:
        space: logit
        min: 0.0
        max: 1.0
      max_grad_norm:
        space: linear
        min: 0.0
        max: 1.0
      ent_coef:
        space: log
        min: 1.0e-05
        max: 0.1
      batch_size:
        space: pow2
        min: 128
        max: 1024
        search_center: 128
      minibatch_size:
        space: pow2
        min: 128
        max: 1024
        search_center: 128
      forward_pass_minibatch_target_size:
        space: pow2
        min: 128
        max: 1024
        search_center: 128
      bptt_horizon:
        space: pow2
        min: 4
        max: 8
        search_center: 8
      total_timesteps:
        space: linear
        search_center: 100000.0
        min: 100000.0
        max: 100000000.0
        is_int: true
    agent:
      observation_encoder:
        normalize_features:
          space: linear
          is_int: true
          min: 0
          max: 1
      fc:
        layers:
          min: 1
          max: 2
        output_dim:
          search_center: 32
          min: 32
          max: 64
      core:
        rnn_num_layers:
          min: 1
          max: 2
        rnn_size:
          search_center: 32
          min: 32
          max: 64
eval:
  num_envs: 10
  num_episodes: 10
  max_time_s: 60
  policy_agents_pct: 0.5
  policy_uri: ${data_dir}/pufferlib/${experiment}
  baseline_uris: []
device: cpu
vectorization: multiprocessing
cmd: sweep
experiment: mac.carbsweep.5
data_dir: ./train_dir
dashboard: true
seed: 1
torch_deterministic: true

wandb: Currently logged in as: daveey (metta-research). Use `wandb login --relogin` to force relogin
wandb: WARNING Tried to auto resume run with id ja3r94t4 but id mac.carbsweep.5 is set.
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /Users/daveey/code/metta/wandb/run-20240923_211250-mac.carbsweep.5
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run mac.carbsweep.5
wandb: ⭐️ View project at https://wandb.ai/metta-research/metta
wandb: 🚀 View run at https://wandb.ai/metta-research/metta/runs/mac.carbsweep.5
wandb.init() has already been called, ignoring.
╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ /Users/daveey/code/metta/tools/run.py:54 in main                                                 │
│                                                                                                  │
│   51 │   │   │   play(cfg, dashboard)                                                            │
│   52 │   │                                                                                       │
│   53 │   │   if cfg.cmd == "sweep":                                                              │
│ ❱ 54 │   │   │   run_sweep(cfg, dashboard)                                                       │
│   55 │                                                                                           │
│   56 │   except KeyboardInterrupt:                                                               │
│   57 │   │   os._exit(0)                                                                         │
│                                                                                                  │
│ /Users/daveey/code/metta/rl/carbs/carb_sweep.py:37 in run_sweep                                  │
│                                                                                                  │
│    34 │   sweep_id = None                                                                        │
│    35 │   if cfg.sweep.resume:                                                                   │
│    36 │   │   try:                                                                               │
│ ❱  37 │   │   │   _dashboard.log(f"Loading previous sweep {cfg.experiment}...")                  │
│    38 │   │   │   artifact = wandb.use_artifact(cfg.experiment + ":latest", type="sweep")        │
│    39 │   │   │   sweep_id = artifact.metadata["sweep_id"]                                       │
│    40 │   │   except CommError:                                                                  │
│                                                                                                  │
│ /Users/daveey/code/metta/rl/pufferlib/dashboard.py:100 in log                                    │
│                                                                                                  │
│    97 │   │   self.policy_params = count_params(policy)                                          │
│    98 │                                                                                          │
│    99 │   def log(self, msg):                                                                    │
│ ❱ 100 │   │   with open(self.log_file, 'a') as file:                                             │
│   101 │   │   │   file.write(msg + '\n')                                                         │
│   102 │                                                                                          │
│   103 │   def update_stats(self, stats):                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
TypeError: expected str, bytes or os.PathLike object, not NoneType
